---
title: '[机器学习课程小结][1/10]'
categories:
  - 什么都学一下
  - 学习log
tags:
  - 机器学习
id: 565
date: 2015-05-05 17:29:19
---

![](http://ww3.sinaimg.cn/large/68eb7c93jw1ertx4e3tejj203704bglr.jpg)<span style="color: #000000;">最近在Stanford的网络课程学习了机器学习的课程，Andre Ng讲的灰常不错啊。每种算法对应的实例挺多的，推导也很详细，它基本用matlab实现。就是概率统计数理统计矩阵论方面的知识最近得补一补了，直接应用在编程语言编写某些函数也是挺头疼的，不错还是略有收获。总共20节，听完5节，做个小结。</span>

<span style="color: #000000;">[L1 Introduction]</span>
<span style="color: #000000;"> 凸优化理论 隐式马尔科夫模型 O natation 学习型算法</span>

<span style="color: #000000;">1)Supervised Learning</span>
<span style="color: #000000;"> 回归问题 Regression 分类问题(x|y)(肿瘤大小|癌症良恶) 学习给定了标准答案的样本-&gt;分析新样本的可能性 支持向量机Support Vector Machine</span>
<span style="color: #000000;"> 2)Learning Theory</span>
<span style="color: #000000;"> 3)Unsupervised Learning</span>
<span style="color: #000000;"> 挖掘 聚类分析-&gt;图像分析和处理的应用 像素处理 ICA Algorithm 鸡尾酒问题（不同声源声音分离）</span>
<span style="color: #000000;"> 4)Reinforcement Learning</span>
<span style="color: #000000;"> 决策分析</span>

&nbsp;

> <span style="color: #000000;">马可夫过程</span>

<span style="color: #000000;"> </span>

<span style="color: #000000;">在概率论及统计学中，马可夫过程（英语：Markov process）是一个具备了马可夫性质的随机过程。马可夫过程是不具备记忆特质的（memorylessness）。换言之，马可夫过程的条件概率</span><span style="color: #ff0000;">仅仅与系统的当前状态相关，而与它的过去历史或未来状态，都是独立、不相关的</span>。<span style="color: #000000;">具备离散状态的马可夫过程，通常被称为马可夫链。马可夫链通常使用离散的时间集合定义，又称离散时间马可夫链。具有马尔可夫性质的过程通常称之为马尔可夫过程。</span>

<table style="width: 60%;">
<tbody>
<tr>
<th scope="col"><span style="color: #000000;"> </span></th>
<th scope="col"><span style="color: #000000;">可数或有限的状态空间</span></th>
<th scope="col"><span style="color: #000000;">连续或一般的状态空间</span></th>
</tr>
<tr>
<th scope="row"><span style="color: #000000;">离散时间</span></th>
<td><span style="color: #000000;">在可数且有限状态空间下的<a style="color: #000000;" title="马可夫链">马可夫链</a></span></td>
<td><span style="color: #000000;"><a style="color: #000000;" title="Harris chain">Harris chain</a> (在一般状态空间下的马可夫链)</span></td>
</tr>
<tr>
<th style="width: 10%;" scope="row"><span style="color: #000000;">连续时间</span></th>
<td style="width: 25%;"><span style="color: #000000;"><a style="color: #000000;" title="&quot;Continuous-time">Continuous-time Markov process</a></span></td>
<td style="width: 25%;"><span style="color: #000000;">任何具备马可夫性质的<a class="new" style="color: #000000;" title="连续随机过程">连续随机过程</a>，例如<a style="color: #000000;" title="维纳过程">维纳过程</a></span></td>
</tr>
</tbody>
</table>

## 

<span style="color: #000000;">具有马尔可夫表示的非马尔可夫过程的例子，例如有移动平均时间序列。最有名的马尔可夫过程为马尔可夫链，但不少其他的过程，包括布朗运动也是马尔可夫过程。</span>

&nbsp;

> <span style="color: #000000;">隐马尔可夫模型</span>

&nbsp;

<span style="color: #000000;">隐马尔可夫模型（Hidden Markov Model，HMM）是统计模型，它用来描述一个含有隐含未知参数的马尔可夫过程。</span><span style="color: #ff0000;">其难点是从可观察的参数中确定该过程的隐含参数</span><span style="color: #000000;">。然后利用这些参数来作进一步的分析，例如模式识别。在正常的马尔可夫模型中，状态对于观察者来说是直接可见的。这样状态的转换概率便是全部的参数。而在隐马尔可夫模型中,状态并不是直接可见的，但受状态影响的某些变量则是可见的。</span><span style="color: #ff0000;">每一个状态在可能输出的符号上都有一概率分布。因此输出符号的序列能够透露出状态序列的一些信息。</span>

<span style="color: #000000;">隐马尔可夫模型最初是在20世纪60年代后半期Leonard E. Baum和其它一些作者在一系列的统计学论文中描述的。HMM最初的应用之一是开始于20世纪70年代中期的语音识别。 在1980年代后半期，HMM开始应用到生物序列尤其是DNA的分析中。此后，在生物信息学领域HMM逐渐成为一项不可或缺的技术。</span>

<span style="color: #000000;">HMM有三个典型(canonical)问题:</span>
<span style="color: #000000;"> 已知模型参数，计算某一特定输出序列的概率.通常使用forward算法解决.</span>
<span style="color: #000000;"> 已知模型参数，寻找最可能的能产生某一特定输出序列的隐含状态的序列.通常使用Viterbi算法解决.</span>
<span style="color: #000000;"> 已知输出序列，寻找最可能的状态转移以及输出概率.通常使用Baum-Welch算法以及Reversed Viterbi算法解决.</span>

&nbsp;

> 凸优化

&nbsp;

<span style="color: #000000;">凸优化，或叫做凸最优化，凸最小化，是数学最优化的一个子领域，研究</span><span style="color: #ff0000;">定义于凸集中的凸函数最小化的问题</span><span style="color: #000000;">。凸优化在某种意义上说较一般情形的数学最优化问题要简单，譬如在</span><span style="color: #ff0000;">凸优化中局部最优值必定是全局最优值</span>。<span style="color: #000000;">凸函数的凸性使得凸分析中的有力工具在最优化问题中得以应用，如次导数等。</span>
<span style="color: #000000;"> 凸优化应用于很多学科领域，诸如自动控制系统，信号处理，通讯和网络，电子电路设计，数据分析和建模，统计学(最优化设计），以及金融。在近来运算能力提高和最优化理论发展的背景下，一般的凸优化已经接近简单的线性规划一样直捷易行。许多最优化问题都可以转化成凸优化（凸最小化）问题，例如求凹函数f最大值的问题就等同于求凸函数 -f最小值的问题。</span>

<span style="color: #000000;">凸优化（凸最小化）问题可以用以下几种方法求解：</span>
捆集法
<span style="color: #ff0000;">次梯度法</span>
<span style="color: #000000;">内点法</span>

<span style="color: #000000;">[L2]Linear Regression线性回归 Gradient descent梯度下降 Normal equations正规化方程组（都需要矩阵变换知识）</span>

<span style="color: #000000;">{(x(i), y(i)); i =1, . . . , m}称作包含m个训练样本的训练集合</span>

![](http://ww1.sinaimg.cn/large/68eb7c93gw1ertvgzf951j20h70czgmk.jpg)

<span style="color: #000000;">h是假设的模型，可以带有一定先验知识，也可以是推测类型。学习算法通过学习训练集合的样本，得到h假设中的一定参数，在新的样本中进行预测和他推理</span>。<span style="color: #ff0000;">若目标变量y是连续的，则这样的学习问题可归为回归问题；若目标变量y是离散的，例如可在{0,1}中取值的垃圾邮件归类、癌症可能性判定问题，则可归为分类问题。</span>

![](http://ww4.sinaimg.cn/large/68eb7c93gw1ertvos0ycwj20b803qjri.jpg)

<span style="color: #000000;">假设h假设是线性的，则假设就是各个参数线性的代数和。其中θ和x均是向量。x中各向量分量是各自变因素的取值，θ是各个自变量分量的系数或权重。则机器学习算法的目的就是求这个假设函数h(x)与真实值y的最小值。评估标准J(θ)以各样本和预测值的差的平方的二分之一为准。（二分之一纯粹是为了后期的运算）</span>

![](http://ww4.sinaimg.cn/large/68eb7c93gw1ertvwyw9uwj20en04hq37.jpg)

<span style="color: #000000;">1\. LMS（<span class="Apple-converted-space"> </span>Least mean square） 算法</span>

<span style="color: #000000;">即最小均方算法。算法实质是自适应的纠错学习，在学习中纠正参数θ的取值，直到收敛。其中，运用的最多的是梯度下降算法（Gradient Descent）</span>

<span style="color: #000000;">算法步骤为：</span>

<span style="color: #000000;">a)选取一个初始参数向量，可令其为0向量</span>

<span style="color: #000000;">b)重复查找各方向梯度，沿着梯度下降最快的方向搜索</span>

<span style="color: #000000;">c)直到四周均比局部梯度大的局部最优点</span>

<span style="color: #000000;">算法的数学描述为（该式为原始式，即梯度下降的数学定义）：</span>

![](http://ww3.sinaimg.cn/large/68eb7c93gw1ertwxvy7r7j20a503h0st.jpg)

<span style="color: #000000;">：=与计算机编程中的赋值语句等同。α称作学习比率。则上式中J(θ)对θ求导的数学推导式为：</span>

![](http://ww1.sinaimg.cn/large/68eb7c93jw1ertx3pvtwhj20k70aw0ui.jpg)

<span style="color: #000000;">这里更新最上式，对于</span><span style="color: #ff0000;">单个训练样本 </span>

![](http://ww3.sinaimg.cn/large/68eb7c93jw1ertxdj12m5j20ef02k0sy.jpg)

<span style="color: #000000;">对</span><span style="color: #ff0000;">所有训练样本<span style="color: #000000;">，沿着梯度方向不断计算直到收敛</span></span>

![](http://ww3.sinaimg.cn/large/68eb7c93jw1ertxfgsbo8j20jb067mxy.jpg)

<span style="color: #000000;">该方法中，每一次梯度的计算都用到了每个训练样本，这种梯度下降的方法称为</span> <span style="color: #ff0000;">批梯度下降<span style="color: #000000;">。由于这里的最优化问题只有一个全局，因此这里的局部梯度最优点即是全局的梯度最优点。此外还可以采取<span style="color: #ff0000;">随机梯度下降</span>，每次梯度的计算选取随机的样本点，这样收敛的速度更快，但可能永远不会收敛到最优点，但会集中在最优点附近。因而在训练样本较大时可以采用随机梯度下降。这是随机梯度下降算法的数学表达：![](http://ww2.sinaimg.cn/large/68eb7c93jw1ertxr2dnpkj20pi09omy1.jpg)</span></span><span style="color: #000000;">这是利用批梯度下降算法计算的梯度直到收敛，可以注意到每一次步长是不一样的，步长可在学习中不断调节，越接近最优点步长越短。</span>

![](http://ww3.sinaimg.cn/large/68eb7c93jw1ertxp6yvc3j20is0ewtb2.jpg)<span style="color: #000000;">2.正规化方程</span>

<span style="color: #000000;">梯度下降是一种求取价值函数J(θ)的方法，正规化方程是另一种方案。首先提到的是几个必要的矩阵知识：</span>

<span style="color: #000000;">f与向量A的映射关系中，对f进行求导的定义：</span>

![](http://ww1.sinaimg.cn/large/68eb7c93jw1erty55sgncj20x00800v7.jpg)

<span style="color: #000000;">关于迹的一些定理：</span>

<span style="color: #000000;">迹的乘法交换：</span>

![](http://ww1.sinaimg.cn/large/68eb7c93jw1ertychjkpmj20ko03ydgn.jpg)

<span style="color: #000000;">迹和转置的迹；迹的加法结合；迹的数乘</span>

![](http://ww4.sinaimg.cn/large/68eb7c93jw1ertycuu329j20bg05bdg7.jpg)

<span style="color: #000000;">迹与倒数相关：</span>

![](http://ww4.sinaimg.cn/large/68eb7c93jw1ertyf4n67aj20gl080ab4.jpg)

<span style="color: #000000;">上式中，B是n_m矩阵，f的定义是依赖m_n矩阵的函数f（A）的值等于AB的迹。其中f（A）中的A的含义是A矩阵的每一个矩阵分量。</span>

<span style="color: #000000;">将J(θ)写成矩阵形式，给定的训练集合定义成矩阵X（m*n）m是训练样本个数，n是每个训练样本的自变量分量。</span>

![](http://ww1.sinaimg.cn/large/68eb7c93jw1erv2wqwhymj20bt06vt91.jpg)

<span style="color: #000000;">则已知的结果也可以用矩阵表示，或者说是向量表示</span>

![](http://ww2.sinaimg.cn/large/68eb7c93jw1erv2xjw1fgj207e06m0sv.jpg)

<span style="color: #000000;">假设h和y的向量差为</span>

![](http://ww3.sinaimg.cn/large/68eb7c93jw1erv2z16c93j20iw0a33zm.jpg)

<span style="color: #000000;">J(θ)则可以表示为：</span>

![](http://ww3.sinaimg.cn/large/68eb7c93jw1erv2zsi8x8j20mc05qdgh.jpg)

<span style="color: #000000;">可以看到，h-y利用了向量内乘的方式，即向量乘以向量的转置，得到的实际上是个一维矩阵即一个数。为了找到J(θ)的最小值，可利用矩阵的求导，并参考上式中迹与倒数相关第三条：</span>

![](http://ww4.sinaimg.cn/large/68eb7c93jw1erv36i7jh1j20p20dhq5v.jpg)

<span style="color: #000000;">第三步骤中，利用到实数的迹也是它本身，第四步中用到了迹和迹的转置，第五步中的方法是利用迹与倒数相关第三条，将A的转置设为θ，B=B的转置=X的转置*X C=单位阵，带入公式即可。令上式为零，则求出导数为零时θ的取值。得到的方程称为正规化方程：</span>

![](http://ww1.sinaimg.cn/large/68eb7c93jw1erv3chess6j20wy06babl.jpg)

> 梯度下降

<span style="color: #000000;">梯度下降法，基于这样的观察：如果实值函数 F(x)在点 (a,f(a)) 处可微且有定义，那么<span style="color: #ff0000;">函数 F(x)在 a点沿着梯度相反的方向 -▽ F(a) 下降最快</span>。</span>

<span style="color: #000000;">因而，如果</span>![\mathbf{b}=\mathbf{a}-\gamma\nabla F(\mathbf{a})](http://upload.wikimedia.org/math/3/f/f/3ff0cdc9fd95f11558270cd4db06ac28.png)<span style="color: #000000;">,对于 ![\gamma&gt;0](http://upload.wikimedia.org/math/e/2/4/e243b8724ee0860a2b24f48b7c5c2360.png) 为一个够小数值时成立，那么 ![F(\mathbf{a})\geq F(\mathbf{b})](http://upload.wikimedia.org/math/9/b/b/9bb1b9d10c2499804ac52aaf0aacd8fe.png)。</span>

<span style="color: #000000;">考虑到这一点，我们可以从函数 ![F](http://upload.wikimedia.org/math/8/0/0/800618943025315f869e4e1f09471012.png) 的局部极小值的初始估计 ![\mathbf{x}_0](http://upload.wikimedia.org/math/c/d/5/cd5146b777ca8406c51d8ba8da7fe901.png) 出发，并考虑如下序列 ![\mathbf{x}_0, \mathbf{x}_1, \mathbf{x}_2, \dots](http://upload.wikimedia.org/math/e/3/6/e36041cdf241095ff7a0e0169a8d405e.png) 使得</span>

<dl><dd><span style="color: #000000;">![\mathbf{x}_{n+1}=\mathbf{x}_n-\gamma_n \nabla F(\mathbf{x}_n),\ n \ge 0.](http://upload.wikimedia.org/math/8/b/0/8b0e3f1c41429f48f4788cfee9fe57ee.png)</span></dd></dl>

<span style="color: #000000;">因此可得到</span>![F(\mathbf{x}_0)\ge F(\mathbf{x}_1)\ge F(\mathbf{x}_2)\ge \cdots,](http://upload.wikimedia.org/math/3/e/a/3ea251fcbc5ebc8b22f515e171570797.png)如果顺利的话序列 ![(\mathbf{x}_n)](http://upload.wikimedia.org/math/c/a/c/cac7a8ab62b2a332af1981678d0504e3.png) 收敛到期望的极值。<span style="color: #000000;">注意每次迭代_步长_ ![\gamma](http://upload.wikimedia.org/math/3/3/4/334de1ea38b615839e4ee6b65ee1b103.png) 可以改变。假设 ![F](http://upload.wikimedia.org/math/8/0/0/800618943025315f869e4e1f09471012.png) 定义在平面上，并且函数图像是一个碗型。蓝色的曲线是等高线，即函数 ![F](http://upload.wikimedia.org/math/8/0/0/800618943025315f869e4e1f09471012.png) 为常数的集合构成的曲线。红色的箭头指向该点梯度的反方向。（一点处的梯度方向与通过该点的等高线的垂线。沿着梯度_下降_方向，将最终到达碗底，即函数 ![F](http://upload.wikimedia.org/math/8/0/0/800618943025315f869e4e1f09471012.png) 值最小的点。</span>

![](http://upload.wikimedia.org/wikipedia/commons/thumb/7/79/Gradient_descent.png/350px-Gradient_descent.png)

&nbsp;