---
title: 机器学习要点检索--基于图解机器学习
date: 2017-02-27 16:54:19
tags:
---

# 机器学习任务

**判别式（识别式）学习**：通过预测数据生成对联合概率**p(x,y)**进行模式识别的方法·

**产生式学习**：利用机器学习方法对训练集**p(y|x)**进行学习的过程

> 可能有拥有足够的信息来很好的解决一个感兴趣的特定问题，但却没有足够的信息来解决一个一般性问题。

--------

若果生成概率已知，则可以推出后验概率；反之，则不能。

$$p(y\mid x)=\frac{p(x,y)}{p(x)}=\frac{p(x,y)}{\sum _{y}p(x,y)}$$

--------

**统计派（频率派）**：将模式Θ(模型参数)作为决定论的变量，使用训练样本${(x_{i},y_{i})}_{i=1}^{n}$对Θ进行学习。(e.g.利用最大似然π)      

其主要**研究课题**是如何有训练集得到高精度的模式Θ。      

$$\underset{\theta}{max}\prod_{i=1}^{y}q(x_{i},y_{i},\theta )$$

**贝叶斯派** ：将模式Θ作为概率变量，对其先验概率p(Θ)加以考虑，计算与训练集相对应的后验概率p(Θ|D).并利用贝叶斯公式求解后验概率

$$p(\theta \mid D)=\frac{p(D\mid \theta )p(\theta )}{p(D)}=\frac{\prod_{i=1}^{n}q(x_{i},y_{i}\mid \theta )p(\theta )}{\int \prod_{i=1}^{n}q(x_{i},y_{y}|\theta )p(\theta )d\theta }$$

贝叶斯学派的**主要任务**是如何通过各种方式精确计算后验概率。

------

>  只在训练集的输入样本附近对函数进行近似，可以减轻维数灾难的影像

# 学习模型

- *线性模型**--典型的代表是$$f_{\theta}(x)=\theta_{1}x+\theta_{0}$$对面积-房价进行一元一次线性拟合

  标准形式为：$f_{\theta }(x)=\sum_{j=1}^{b}\theta_{j}\phi_{j}(x)=\theta ^{\top }\phi (x)$,其中$\theta_{j}$是参数向量（待解求的参数集合）$\theta=(\theta_{1},\theta_{2}...\theta_{b})^{\top }$的第j个因子；$\phi_{j}(x)$ 是基函数（决定了拟合的模式 以特征为基础可以自由决定衍生的多项式、函数模式$ln x$、$\frac{1}{x}$等，一般来说都是多项式，即$\phi_{j}(x)=(1,x,x^2...x^{b-1})^{\top }$;又或者是三角多项式形式，即$\phi_{j}(x)=(1,sin x,cos x,sin 2x,cos 2x...sin mx,cos mx)^{\top }$）

**当不止考虑一个维度的因素（特征）时，输入维度太多易造成维度爆炸。**

- **核模型**--典型代表四高斯核$K(x,c)=e^{-\frac{\left \| x-c \right \|^2}{2h^2}}$，可理解为欧几里得空间映射到核函数空间中，在函数设计时可以依赖样本$$\{(x_{i},y_{i})\}_{i=1}^{n}$$。式中，h和c分别是带宽和均值。高斯核模型一般只能对样本附近的函数进行近似.

  标准形式为：$f_{\theta}(x)=\sum_{j=1}^{n}\theta_{j}K(x,x_j)$ 其中$K(x,x_j)$是二元核函数，以线性相加所有样本的方式定义。

**参数的个数不依赖于输入变量的维度d，而只有训练样本的数量m决定。**

- **层级模型**--典型代表是神经网络,尤其是不完全映射的神经网络.

  标准形式为:$f_{\theta}(x)=\sum_{j=1}^{b}\alpha _{j}\phi (x;\beta _{j})$  其中$\phi (x;\beta _{j})$是带有参数向量β的基函数。即，整个模型的参数为$\theta=(\alpha ,\beta _{1}^\top ,...,\beta _{b}^\top )^\top $其中，α是层次模型线性形式的参数向量。

  ​

  ------

  # 最小二乘法

  入门的入门就是最小二乘了，类似二范数，目标是最小化所有样本与其预测值差的平方和：

  **敲黑板**，上边表达的就是代价函数，即下式：

  $$J_{LS}(\theta )=\frac{1}{2}\sum_{i=1}^{n}(f_{\theta }(x_{i})-y_{i})^2$$

  解求目标是：

  $$\widehat{\theta_{LS}}=\underset{\theta }{argmin}J_{LS}(\theta )$$

  再次**敲黑板**，这句话包含很多意思

  1. 使用最小二乘算法，解算的目标是：当代价函数求得最小的时候参数$\theta$的值
  2. 代价函数的最小值是多少我不关心，我只关心代价函数求得最小时参数向量的值是多少--即解求的目标不是函数的最值，是函数的极值点。
  3. 常用在线性函数最值的求解，易过拟合

  对于解求一元函数，窝们知道应该对函数求导求取零值点；而对于向量（可视为多元函数），求取各参量的偏导求解零值亦可求取极值。其偏导为：

  ​