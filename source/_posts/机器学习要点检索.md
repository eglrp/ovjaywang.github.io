---
title: 机器学习要点检索--基于图解机器学习
date: 2017-02-27 16:54:19
tags:
---

# 机器学习任务

**判别式（识别式）学习**：通过预测数据生成对联合概率**p(x,y)**进行模式识别的方法·

**产生式学习**：利用机器学习方法对训练集**p(y|x)**进行学习的过程

> 可能有拥有足够的信息来很好的解决一个感兴趣的特定问题，但却没有足够的信息来解决一个一般性问题。

--------

如果生成概率已知，则可以推出后验概率（所有y可能性的联合概率加和计算x的概率）；反之，则不能。

$$p(y\mid x)=\frac{p(x,y)}{p(x)}=\frac{p(x,y)}{\sum _{y}p(x,y)}$$

--------

**统计派（频率派）**：将模式θ(模型参数)作为决定论的变量，使用训练样本${(x_{i},y_{i})}_{i=1}^{n}$对Θ进行学习。(e.g.利用最大似然π)      

其主要**研究课题**是如何有训练集得到高精度的模式Θ。      

$$\underset{\theta}{max}\prod_{i=1}^{y}q(x_{i},y_{i},\theta )$$

**贝叶斯派** ：将模式Θ作为概率变量，对其先验概率p(Θ)加以考虑，计算与训练集相对应的后验概率p(Θ|D).并利用贝叶斯公式求解后验概率

$$p(\theta \mid D)=\frac{p(D\mid \theta )p(\theta )}{p(D)}=\frac{\prod_{i=1}^{n}q(x_{i},y_{i}\mid \theta )p(\theta )}{\int \prod_{i=1}^{n}q(x_{i},y_{y}|\theta )p(\theta )d\theta }$$

贝叶斯学派的**主要任务**是如何通过各种方式精确计算后验概率。

------

>  只在训练集的输入样本附近对函数进行近似，可以减轻维数灾难的影响

# 学习模型

- **线性模型**--典型的代表是$$f_{\theta}(x)=\theta_{1}x+\theta_{0}$$对面积-房价进行一元一次线性拟合

  标准形式为：$f_{\theta }(x)=\sum_{j=1}^{b}\theta_{j}\phi_{j}(x)=\theta ^{\top }\phi (x)$,其中$\theta_{j}$是参数向量（待解求的参数集合）$\theta=(\theta_{1},\theta_{2}...\theta_{b})^{\top }$的第j个因子；$\phi_{j}(x)$ 是基函数（决定了拟合的模式 以特征为基础可以自由决定衍生的多项式、函数模式$ln x$、$\frac{1}{x}$等，一般来说都是多项式，即$\phi_{j}(x)=(1,x,x^2...x^{b-1})^{\top }$;又或者是三角多项式形式，即$\phi_{j}(x)=(1,sin x,cos x,sin 2x,cos 2x...sin mx,cos mx)^{\top }$）

**当不止考虑一个维度的因素（特征）时，输入维度太多易造成维度爆炸。**

- **核模型**--典型代表四高斯核

  $$K(x,c)=e^{-\frac{\left \| x-c \right \|^2}{2h^2}}$$

  可理解为欧几里得空间映射到核函数空间中，在函数设计时可以依赖样本$$\{(x_{i},y_{i})\}_{i=1}^{n}$$。式中，h和c分别是带宽和均值。高斯核模型一般只能对样本附近的函数进行近似.

  标准形式为：$f_{\theta}(x)=\sum_{j=1}^{n}\theta_{j}K(x,x_j)$ 其中$K(x,x_j)$是二元核函数，以线性相加所有样本的方式定义。

**参数的个数不依赖于输入变量的维度d，而只有训练样本的数量m决定。**

- **层级模型**--典型代表是神经网络,尤其是不完全映射的神经网络.

  标准形式为:$f_{\theta}(x)=\sum_{j=1}^{b}\alpha _{j}\phi (x;\beta _{j})$  其中$\phi (x;\beta _{j})$是带有参数向量β的基函数。即，整个模型的参数为$\theta=(\alpha ,\beta _{1}^\top ,...,\beta _{b}^\top )^\top $其中，α是层次模型线性形式的参数向量。

  ​

  常见的层级模型是卷积和神经网络。

  ------

  # 最小二乘法

  入门的入门就是最小二乘了，类似二范数，目标是最小化所有样本与其预测值差的平方和：

  **敲黑板**，上边表达的就是代价函数，即下式：

  $$J_{LS}(\theta )=\frac{1}{2}\sum_{i=1}^{n}(f_{\theta }(x_{i})-y_{i})^2$$

  其中，i是具体样本编号，n是样本个数，LS即least square最小二乘。

  解求目标是：

  $$\widehat{\theta_{LS}}=\underset{\theta }{argmin}J_{LS}(\theta )$$

  其中$\widehat{\theta_{LS}}$表达对参数的预测值。再次**敲黑板**，这句话包含很多意思

  1. 以最小二乘法表达代价函数，解算的目标是：当代价函数求得最小的时候参数$\theta$的值
  2. 代价函数的最小值是多少我不关心，我只关心代价函数求得最小时参数向量的值是多少--即解求的目标不是函数的最值，是函数的极值点。
  3. 常用在线性函数最值的求解，配合**SVD**(范式函数)或**梯度下降法**解求$\theta$的值 两者的内涵是一致的
  4. 需提前大致知道样本的模式(即基函数的确定，对多项式来说是最高几次？对三角函数式来说是基函数的振幅和频率的确定——基函数次数过高或项数过多易造成过拟合

  ## 正态方程解法(Normal Equation)

  对于解求一元函数，窝们知道应该对函数求导求取零值点；而对于向量（可视为多元函数），求取各参量的偏导求解零值亦可求取极值。对广义线性模式的预测函数

  $$f_{\theta }(x_{j})=\theta_{b-1}x^{b-1}_{j}+...+\theta_{2}x^{2}_{j}+\theta_{1}x_{j}+\theta_{0}x_{j}=\Theta ^{\top }\phi (x)$$

  来说，参数的偏导形式为：

  $$\bigtriangledown_{\theta}J_{LS}=\frac{\partial J_{LS}}{\partial \theta }=\frac{1}{2}\cdot 2\cdot\Phi ^{\top }(\Phi \Theta -y)=\Phi^{\top }\Phi \Theta ^{\top }-\Phi ^{\top}y$$

  其中，$\Phi $为基函数与样本点组成的n×b矩阵，

  $$\Phi =\begin{pmatrix}
  \phi\_{1}(x\_{1}) &\cdots   & \phi\_{b}(x\_1)\\
   \vdots & \ddots  & \vdots \\
  \phi\_{1}(x\_n) &\cdots & \phi\ _{b}(x\_{n})
  \end{pmatrix}$$

  $\Theta$为b ×1 维参数向量参数向量。令偏导为0即可解求参数向量$\theta$的具体值：

  **$$\widehat{\theta _{LS}}=(\Phi^{\top }\Phi)^{-1} \Phi^{\top }y$$**

  上式对广义线性模式的基函数与参数向量的组合均适用。式中的矩阵逆运算，可通过**SVD**分解求得，因而如果样本过多（m值过大），维度过大(n值过大)，则复杂度较高，计算消耗较大（O(n^3)的复杂度）。

  若换用核模型，$$f_{\theta}(x)=\sum_{j=1}^{n}\theta _{j}K(x,x_j)$$,可认为直接把$\Phi$替换定义的核矩阵，即可求得核模型的最小二乘解。上式$\Phi$可替换为

  $$\kappa = \begin{pmatrix}

  K(x\_{1},x\_{1}) &\cdots   & K(x\_{1},x\_{n})\\\\

   \vdots & \ddots  & \vdots \\\\

  K(x\_n,x\_1) &\cdots & K(x\_{n},x\_{n})

  \end{pmatrix}$$

  **注意notes**  当矩阵不可逆，及其可能两个特征过于**接近**或特征过少样本过多

  ------

  > 最小二乘法输出的预测值时由$	\Re(\Phi )$的正摄投影得到。其中，$\Re$表达$\Phi$的值域，$\Phi$是上式基函数矩阵与具体样本构成的n×b矩阵

  -----

  ## 梯度下降解法

  ### 梯度下降基本原理

  计算线性回归时，为了更快收敛更节省计算资源，设计**梯度下降法**——给定某个初值，以一定的**步长α**（学习率）向**梯度的方向**（函数向局部极小值 下降最快的方向）趋近迭代的算法。

  - 该方式主要针对线性空间凸函数，即有可求极值点
  - 该方法需要同时更新所有的参数向量，计算本次 参数向量统一使用上一次的数据
  - 设置合理的阈值能更快收敛

  对向量参数$\theta$的每个具体参数，其迭代的方式为：$$\theta_{j}  += \theta_{j} - \alpha \frac{\partial }{\partial \theta _{j}}J_{LS}(\theta )$$

  其中α是**学习率**，后边一坨是**代价函数对具体参数向量的偏导**。学习率决定了趋近的速率，过小迭代次数过多收敛过慢，过大易造成远离极值点。对假设模型为线性函数形如上式$f_{\theta }(x_{j})=\theta_{b-1}x^{b-1}_{j}+...+\theta_{2}x^{2}_{j}+\theta_{1}x_{j}+\theta_{0}x_{j}=\Theta ^{\top }\phi (x)$的偏导数，其具体偏导为：

  ​

  $$ \frac{\partial }{\partial \theta }\frac{1}{2}(f_{\Theta }(X)-Y)^2=2\cdot \frac{1}{2}\cdot(h_{\theta}(X)-Y)\frac{\partial }{\partial \theta _j}(\sum_{i=0}^{n}\theta _{i}x_{i}-Y)=(h_{\theta}(X)-Y)x_j $$

- 其中j是参数向量某个具体位置，而$x_j$是其对应位置的基函数的值而非样本

- $\phi$是基函数，$X$ Y分别是样本向量$X=\{x_1,x_2,...x_m\}$和样本结果实际值向量$Y=\{y_1,y_2...y_m\}$

- i是基函数数量

- 举个栗子。当假设基函数为$\phi(x)={(x,1)}$即$f(x)=\theta_1x_1+\theta_0$其迭代时具体值为：

  $$\theta_0 += \theta_0 - \alpha\frac{1}{m}\sum_{m}^{i=1}(h_{\theta}(x_i)-y_i)$$

  $$\theta_1 += \theta_1 - \alpha\frac{1}{m}\sum_{m}^{i=1}((h_{\theta}(x_i)-y_i)x_i)$$

  ------

  ### 梯度下降的注意事项

  梯度下降主要由三种更新方式：

  1. 以最最基本的梯度下降原理，每一次带入**所有**的样本数据解求更新偏导数-batch批量梯度下降

  2. 数据量过多，每次**随机**抽一个带入解算偏导数，则上式解算每个偏导的函数变为

     $$666$$--随机梯度下降

  3. 结合以上两种方式，每次迭代抽取**一小波**样本更新偏导数--mini-batch 随机批量梯度下降

  一般来说，梯度下降为了保证计算效率和迭代效率，选择第三种比较划算。      

  ​      

  梯度下降中，选择**合适的α**比较重要。

  - 过小导致收敛过慢
  - 过大可能无法收敛或越出该局部最小值
  - 需要对多个α进行测试--出现代价函数增大应减小学习率；若迭代收敛过慢则增大学习率

   选择合适的特征尺度也利于收敛。一般而言，可以选择正则化以保证收敛时的计算更为**顺滑精确**(梯度平缓步长合适)。**注意**：特征尺度不能解决设计矩阵不可逆、J(θ)不收敛等问题。

  ------

 ### 梯度下降和正太方程比较

 

|                 | Gradient Descent | NORmal Equation |
| :-------------: | :--------------: | :-------------: |
|      需要选择α      |        ✔         |        ✘        |
|     需要迭代计算      |        ✔         |        ✘        |
|     需要计算逆矩阵     |        ✘         |        ✔        |
| 当维度n和样本m过大时计算顺利 |        ✔         |        ✘        |

  ------

# Logistics回归（逻辑回归）

## **这是一个分类算法**

